[
    {
        "url": [
            "https://www.youtube.com/watch?v=22tkx79icy4"
        ],
        "title": [
            "RAG | САМОЕ ПОНЯТНОЕ ОБЪЯСНЕНИЕ!"
        ],
        "description": [
            "САМОЕ ПОНЯТНОЕ ОБЪЯСНЕНИЕ RAG СИСТЕМ!\n\nПереходи в наше коммьюнити в тг AI RANEZ - https://t.me/+ExiaDZ5sN1k0NWEy\n\nВ этом подробном гайд‑видео я раскрываю всё, что нужно знать о RAG (Retrieval Augmented Generation) — передовом подходе, который выводит большие языковые модели (LLM, GPT‑4, ChatGPT и др.) на новый уровень, добавляя к их генеративным возможностям живую, актуальную базу знаний. Вы увидите, как на практике связать эмбеддинги, векторное хранилище, retriever и generator, чтобы буквально «подпитать» модель свежим контентом и получить точные, аргументированные ответы без «галлюцинаций». Я пошагово показываю архитектуру, объясняю ключевые нюансы (latency, стоимость, обновление данных), визуально скетчу процесс, разбираю реальные сценарии применения: чат‑бот поддержки, интеллектуальный поиск по корпоративным документам, персонализированный ассистент и многое другое. Параллельно я делюсь лайфхаками, где RAG приносит максимальную пользу, а где лучше отказаться от него в пользу классических решений. После просмотра у вас будет чёткая дорожная карта: как спроектировать, собрать и оптимизировать собственную RAG‑систему под ваш use case. Буду рад обратной связи: напишите, насколько удобен формат «живых» скетчей и что улучшить в следующих выпусках — уже готовлю видео, где вместе шаг‑за‑шагом построим рабочий прототип. Ставьте лайк, подписывайтесь и делитесь идеями тем, которые хотите увидеть на канале.\n\nRAG, Retrieval Augmented Generation, RAG tutorial, RAG система, LLM, большие языковые модели, векторная база, embeddings, LangChain, LlamaIndex, GPT‑4, ChatGPT, AI разработка, generative AI, поиск по документам, knowledge base bot, машинное обучение, NLP, vector database, искусственный интеллект, обучение ИИ, generative AI tutorial, build RAG system, AI use cases, корпоративный AI, AI for business\n\n#RAG #RetrievalAugmentedGeneration #ИИ #ии #ai #LLM #GenerativeAI #LangChain #VectorDatabase"
        ],
        "audio_path": [
            "data/audio/22tkx79icy4.mp3"
        ],
        "text": [
            "RAC – Retrieval Augmented Generation. Что это такое? Как это работает? Зачем это нужно? Приветствую! Сегодня я объясню вам, что же такое RAC, простыми словами. Я наглядно покажу, как все это работает, какие проблемы RAC решает и зачем это вообще надо. RAC – это прием, при котором перед каждым вызовом LLM мы достаем, так скажем, релевантные чанкс, куски внешних данных, внешние данные могут храниться в дейтабазе, и добавляем их в Augmented Prompt. Сейчас, чуть попозже, я вам все это объясню, покажу весь workflow, как это работает, но сейчас поймите, что RAC – это просто прием, при котором перед каждым вызовом нейронки, перед каждым промптом, который мы даем в LLM, мы сначала достаем просто дату из датабазы, соответственно, потом ее просто скармливаем в нейронку в виде контекста. Это такое очень high-level представление о RAC-системе. У LLM есть пару открытых проблем. Первое – контекстное окно. Соответственно, контекстные окна еще не настолько большие, чтобы запихивать туда кучу информации, и из-за этого нейронка начинает теряться, забывать, и происходит, соответственно, этот затуп, и мы не можем поместить огромную, допустим, датабазу в наш промпт. Это первая проблема. Вторая проблема – это no source, или же нет достоверного источника, откуда, соответственно, брать правильную информацию, которая нужна именно в вашем случае, например, из вашей датабазы. Может быть так, что тематика, которая вам нужна, не была в датасете при тренировке LLM в виде chat.gpt или DeepSeek, неважно. И еще одна проблема – это out of date. Зачастую бывает, что информация в LLM уже устаревшая, потому что ее тренировали, допустим, полгода назад, и информация уже устарела, и она является неверной из-за этого. Поэтому у нас есть такие три основные проблемы. Контекстное окно – no source. Нету достоверного источника, откуда берется информация, и, соответственно, устаревшая информация. Рак системы, они решают эти три проблемы, и сейчас я вам расскажу, как это все работает дальше. Сначала рассмотрим ситуацию взаимодействия юзера просто с LLM, без всяких датабаз, без всяких рак систем. У нас есть юзер, вот здесь у нас есть юзер LLM. Как происходит взаимодействие между юзером и Large Language Models? У юзера есть какой-то вопрос, он пишет это в промпт, то есть у нас есть промпт, он отправляет промпт в Large Language Models. В промпте может быть все что угодно, например, какая самая близкая планета к Солнцу. Дальше LLM получает на input этот промпт, то есть получает этот запрос. Через небольшое количество времени отвечает на этот запрос, исходя из тех данных, на которых она была натренирована. Не ссылаясь на источник, не подтверждая, что эта информация обновлена, она просто дает ответ. Дальше перейдем к тому, как работают рак системы, потому что это сильно отличается от простого взаимодействия юзера с LLM. Рассмотрим пример, где юзер пишет промпт, то есть у нас опять же есть юзер, он пишет какой-либо промпт, вот здесь промпт. Например, это может быть опять же, какая планета самая близкая к Солнцу, но здесь и начинается отличие. В игру входит Small Embedded Model, сейчас я вам объясню, что это такое. Как правило, это такая маленькая модель, которая просто конвертит ваш промпт, а именно текст, в вектора. То есть трансформирует ваш текст в численные вектора, их еще называют Embedded Vectors. Это обычно очень маленькие модели, состоящие из Encoder, Encoder – это часть трансформера. Для тех, кто не знает, Large Language Models построены на архитектуре трансформеров, это если прям сильно упрощать, то есть у нас есть трансформеры, и на них построено LLM. Сами трансформеры состоят из Encoder части и Decoder. Соответственно, эти маленькие Small Embedded Models построены именно на базе Encoder, то есть состоят из Encoder. Это нужно для того, чтобы обычные слова, обычный текст трансформировать в Vectors. Как правило, эти Small Embedded Models – это BERT-стайл-энкодер, вы подробно можете прочитать про эту архитектуру в интернете, все подробно расписано, сейчас я застрять на этом свое внимание не буду. Дальше, в чем самая главная фишка Rack? В том, что у нас есть какая-то датабаза, это может быть все что угодно, это может быть PDF-файл, какая-то книга, но обычно это просто датабаза с информацией, допустим, информация о вашем продукте, описание, спецификация и так далее. Эта датабаза делится на маленькие chunks, то есть на маленькие куски, пласт информации делится просто на маленькие куски. Дальше, эти куски трансформируются уже в Vectors, это все нужно для быстрого поиска, то есть эти chunks, они трансформируются в Vectors. Соответственно, вся датабаза у нас трансформирована в Vectors. На данный момент мы имеем Prompt Vector, то есть Vector, который состоит из Prompt User, и датабаза, которая уже разделена на Vectors. Дальше, у нас происходит сравнение Prompt Vector с Vector, который из датабазы, то есть dbVector. Как вообще эти Vectors выглядят? Давайте я сейчас немного заскетчу это быстренько. Vectors могут выглядеть как угодно, так, так, так, просто набор цифр, грубо говоря. Давайте представим, что вот этот Vector это Prompt Vector, и на этом этапе VectorSearchEngine, они сравнивают эти Vectors. Пример Search Engine – это F-A-I-S-S. Тоже можете почитать информацию в интернете очень много на эту тему. Vectors сравниваются с помощью, соответственно, этого Engine, и если в каком-либо chunk, то есть в этом маленьком кусочке, есть ответ на вопрос юзера, либо есть какая-то информация, связанная с Prompt User, она будет добавлена в контекст далее, но это чуть попозже. Допустим, вот у нас есть схожие Vectors, вот эти два Vectors, это Database Vector. После сравнения, когда Engine уже нашел схожие Vectors, вот эти Vectors из датабазы вдаются в контекст. После вот этого сравнения у нас уже формируется так называемый Augmented Prompt. Давайте я напишу, из чего состоит этот Augmented Prompt. Он состоит из инструкции, это может быть все, что угодно, по типу UAHelp or Assistant, то есть просто инструкция для LLM, как ей надо работать. Дальше у нас идет, соответственно, этот контекст. Контекст мы получили из вашей датабазы, заточенную на ваш Task, там может быть спецификация вашего продукта, все что угодно. И этот контекст мы получили, соответственно, из датабазы с помощью сравнения Vectors, которые состоят из Prompt User и из chunk в датабазе. Эти Vectors сравнили, появился контекст, который дается в Augmented Prompt. В третьем у нас идет сам Prompt, в который юзер писал, в нашем случае, какая планета самая близкая к Солнцу. Весь этот Augmented Prompt дается на Input в LLM. Это может быть ChatGPT, Cloud, Gemini, это может быть любая Large Language Models, может быть OLAMO, в общем их сотня, это все не имеет значения. LLM в запросе имеет уже контекст, и LLM в запросе имеет, соответственно, инструкцию, как ей взаимодействовать. Она имеет Prompt User, то есть его вопрос, и она уже имеет контекст. Это будет либо ответ на вопрос из датабазы, либо информация, которая связана с этим вопросом. И, исходя из этих трех пунктов, она уже формирует ответ, присылает его обратно юзеру. Важный момент, если юзер дает какой-либо Prompt, и ответа, либо смежной информации с этим Prompt нет в датабазе, LLM выдаст обратно, что она не знает ответ на данный вопрос. На самом деле, это намного лучше, чем LLM просто будет вам нести чушь, давать ложные факты и сведения. В целом, так и работают RAG-системы. Вы можете использовать их в большом количестве случаев. Например, вам нужно, чтобы ваша LLM, а именно это уже будет AI-agent, консультировала клиента по вашему товару. Это может быть какой-то софт, еще что-то там, iPad и так далее. Создаете датабазу, вносите всю информацию о продукте, о вашем продукте, о вашем софте, всю спецификацию, все тонкости вашего товара, все прописываете в датабазу. И теперь AI-agent при общении с клиентом будет давать достоверные сведения о вашем товаре. Получается промпт, допустим, это ваш клиент, он будет давать промпт. Это все будет переводиться, трансформируется в вектора. Вектора будут сравниваться с вашей датабазой. И будут даваться augmented prompts в LLM с контекстом о вашем специфическом товаре. И уже будет ответ для клиента со всеми тонкостями вашего товара. RAG – это вообще суперкрутая вещь. Она фиксит проблему, что у LLM достаточно маленькое контекстное окно. Применения этому RAG очень много. Все ограничивается только, как всегда, вашей фантазией. В следующих видео мы можем разобрать разновидности RAG. Такие как CAG, CAG и так далее. Их достаточно много. Я хочу показать, как создать свою RAG-систему. Я очень надеюсь, что я понятно объяснил RAG. Что у вас теперь не вызывает вопросов эта аббревиатура. И вам полностью понятна данная тема. Если вам понравился данный видеоролик, пожалуйста, поставьте лайк. Это помогает в продвижении данного контента. И напишите в комментариях, что вы хотите увидеть дальше. Всех благодарю за просмотр. Всем удачи, скоро увидимся.\n"
        ]
    },
    {
        "url": [
            "https://www.youtube.com/watch?v=sbMzUOXcyWw"
        ],
        "title": [
            "Fine-tuning, RAG, Llama, prompt-engineering, LLM-арены | Что происходит в LLM"
        ],
        "description": [
            "Записывайтесь на курс по LLM: https://deepschool.ru/llm?utm_source=yt&utm_medium=description&utm_campaign=course-llm5&utm_content=deepschool&utm_term=what's-going-on-in-llm\n\nЕсли вы проспали бум вокруг LLM и хотите разобраться, что сейчас в них происходит простыми словами, от инженера инженеру, мы подготовили для вас видео!\n\nHugging face, библиотека Transformers: https://huggingface.co/docs/transformers/index\n\n00:00:00 | Что такое LLM и как их используют\n00:02:00 | Как сделать своего ассистента дома: модели и дообучение\n00:04:20 | Веб-интерфейсы и видеопамять для обучения и обученные модели\n00:05:20 | Как запускать обученные модели дома и ускорение LLM\n00:07:50 | Минусы LLM и подборы промта\n00:08:50 | RAG: как освежать память модели и общаться с pdf-ками\n00:09:55 | Fine-tuning модели\n00:10:55 | А как выбрать модель? LLM-арены\n00:12:48 | Как научиться работать с LLM\n\n#largelanguagemodels #llm #deeplearning #career #nlp #ai"
        ],
        "audio_path": [
            "data/audio/sbMzUOXcyWw.mp3"
        ],
        "text": [
            "Всем привет, меня зовут Илья Димов, вы на канале DeepSchool. Если вы пропустили всю шумиху вокруг больших языковых моделей, ничего страшного, сейчас нагоним. Поехали. LLM – это класс моделей, которые учатся продолжать текст по заданному началу. Архитектурный сейчас – это почти вся трансформер, который пришел на смену рекуррентным нейросетям. Трансформеры вообще сейчас доминируют во многих областях дип-лернинга, они успешно работают с текстами, с картинками, со звуками и даже с сырыми байтами. Современные языковые модели умеют писать тексты, неотличимые от человеческих, могут даже написать книгу, которыми… нет, я не буду ругать Амазон, это меня засудит джифф-бизнес. Могут даже написать книгу вместе с человеком, а могут и вовсе без человеческого участия. Но это звучит довольно скучно. Что еще? А что еще? Помимо продолжения текста, LLM можно использовать как классификатор. Например, можно подать ей текст и спросить, он грустный или веселый, и она выдаст одно слово, являющееся ответом. Еще можно попросить модель пересказать текст. Например, можно взять несколько отзывов с маркетплейса и попросить модель выделить плюсы и минусы товара. Другое интересное применение – это генерация кода. Например, можно описать сигнатуру функции и ее документацию, а LLM сама допишет тело. Можно даже с помощью LLM решать международные математические олимпиады на уровне серебряного призера. Но какое может быть обсуждение LLM без упоминания ассистентов, кто еще не слышал про чат GPT? Если вы проспали и это, то вкратце – это чат-бот, к которому можно задать вопрос на любую тему и он постарается на него ответить. Можно спросить, как оформить юридический документ, можно спросить вопрос по истории, по этимологии слова, только пожалуйста, не просите их складывать числа, с этим пока очень плохо. Проверка правильности ответа все еще лежит на пользователя. LLM умеет очень искренне и убедительно говорить неправду. А как вообще из LLM получается ассистент? Можно ли это сделать дома и гонять потом его на персональном компьютере под столом? Давайте разбираться. Хорошая новость заключается в том, что идея на обучение собственного ассистента не очень сложная. Для этого нам понадобится модель, данные и железо, чтобы его обучить. Начнем с моделей. В наши дни уже доступно множество LLM по API, например закрытых коммерческих от OpenAI, Google и Antropic. Цены на них совсем не кусачие, от нескольких десятков центов до долларов за миллион токенов, а это между прочим объем войны и мира. Более того, у многих моделей есть мини или лайт версии, которые выдают качество похуже, однако гораздо дешевле. Сильно развито и опенсорс сообщество. Различные опенсорс модели постоянно дообучаются на различных корпусах и выкладываются на сайт Hugging Face, откуда с помощью библиотеки Transformers можно удобно в несколько строчек кода скачать нужную модель и начать ей пользоваться. Все современные модели по типу LLM3 уже залиты в этот репозиторий. Они представлены в разных размерах, 8 миллиардов, 70 миллиардов и даже огромная 400 миллиардная модель. Для сравнения, оригинальная GPT-3 имела в себе 175 миллиардов параметров и обучалась на огромном кластере. Разнообразие моделей на семействе LLM не заканчивается. Есть интересные модели по типу Mixtral, основанные на архитектуре смеси экспертов. Оригинальная модель Mixtral содержит в себе 47 миллиардов параметров, однако за счет архитектуры смеси экспертов она работает гораздо быстрее, чем другие модели подобного объема и тоже дает хорошее качество. Дальше нам нужен датасет качественных инструкций. Инструкция – это текст, который описывает какую-либо задачу или задает вопрос, и ответ. Ответ должен быть качественный, развернутый и тот, который вы хотели бы увидеть на данный вопрос. Из-за того, что языковые модели за счет обучения на огромных корпусах данных уже имеют определенные знания о нашем мире, подобное дообучение позволяет им эффективно использовать эти знания для генерации правильных ответов. После такого дообучения возможности LLM значительно расширяются. Например, можно подать модели какой-нибудь учебник, который рассказывает о том, как переводить с одного языка на другой. При этом сам этот язык модель видела или очень мало, или не видела вовсе. И по набору правил описанных в учебнике модель будет выдавать не самый лучший, но перевод. После этого все просто, мы снимаем небольшой кластер и запускаем обучение. Но что делать, если у вас нет собственного кластера и вы не хотите работать с кодом? Ничего страшного, уже есть веб-интерфейсы, позволяющие драгон-дропом загрузить датасет и запустить обучение. Более того, все они предоставляют возможности поджатия моделей. Например, 8-миллиардную ламу можно уместить всего на 24 гигабайта видеопамяти. А это уже RTX 4090, использовательского сегмента цен. Если амбиции еще больше, можно взять 70-миллиардную модель, которая влезает на 48 гигабайт видеопамяти. Обучение, конечно, будет долгим, однако его можно сделать. После этого ассистент обучен и можно тестировать его. Однако, если на первом шаге в выборе модели вы задумывались, почему бы нам не взять уже готовую, то вы задумывались не зря. В том же репозитории Huggenface уже лежит множество обученных моделей ассистентов, с которыми можно брать и начинать общаться. Если не хватает момент истины, модель или скачена или обучена, как с ней общаться? Запустить ее можно все той же библиотекой Transformers из Huggenface. Для этого желательно видеокарта, но если у вас всего лишь обычный ноутбук, ничего страшного, запускать модели можно и на процессоре. Отсутствие GPU для моделей от 7 до 20 миллиардов параметров уже давно не является преградой. Проект от болгарского разработчика Георгия Гирганова Lama CPP за счет эффективной имплементации на C++ и выжимания всех возможностей современных процессоров, позволяет запускать модели на обычном процессоре с приемлемой скоростью. Например, 13-миллиардная Lama 2 будет выдавать 16 токенов в секунду, чего с головой хватит для личного пользования. А что же такое квантизация и сжатие моделей? Вкратце, обычно параметры модели занимают 2 байта, однако это довольно много для огромных моделей, поэтому их пытаются сжать в 1 байт и даже в несколько бит, чтобы вместить на процессоры. Более того, с такими небольшими типами данных процессор работает быстрее, однако стоит отметить, что все техники сжатия вливаются в некоторую потерю качества модели. Можно мне паузу? У меня до мысли тяжелую сформулировать. Не отстают и техники инференса, например, те же квантизированные модели на инференс будут занимать намного меньше места, чем на обучении, однако, как я уже сказал, это пойдет в ущерб качеству модели. А вот техники сгрузки весов моделей позволяют возместить потери в качестве за счет потери в скорости. Ту же 70-миллиардную модель можно уместить всего в 4 гигабайта видеопамяти, однако большая часть времени уйдет не на полезные вычисления, а на сгрузку и загрузку весов. Различные исследовательские группы по всему миру бьются над тем, чтобы сжать модели все сильнее и не терять качество. Модель даже готова сжаться до 1,5 бит на параметр, чтобы работать у вас. Таким образом, своего личного ассистента можно держать абсолютно локально. Однако, если греться от ноутбука вы не планируете и спокойно относитесь к тому, чтобы посылать свои данные на чужие сервера, вы можете воспользоваться провайдерами LLM, такими как OpenRouter, Fireworks, TogetherAI и другие. Они предоставляют удобное IP, в котором можно спокойно посылать свои запросы почти в любые Open Source LLM. Если же вы не хотите посылать свои данные в сторонние сервисы и у вас есть требования к большой нагрузке, придется раскошелиться на видеокарты и использовать особые фреймворки для инференса, такие как TRT-LLM, VLLM, TGI и другие. Эти фреймворки постоянно выпускают новые оптимизации инференса с учетом современных архитектур GPU и заимствуют друг у друга нововведения, поэтому выбрать можно любой понравившийся. Мы много хвалим модели и рассказываем об их доступности, а есть ли минусы? Конечно есть. Модели они как люди, каждая со своим характером. Одна отвечает длинно, другая коротко, одна грубо, другая нежно. Как можно повлиять на то, как отвечает модель? На помощь приходит промпт, это и есть часть описания инструкции, которая подсказывает модели, каким образом построить свой ответ. Подбор промпта это очень важная задача, хоть многие относятся к ней пренебрежительно, подобрать правильный промпт порой бывает очень сложно, но он при этом может давать заметный буст к качеству. Техники бывают довольно разные, от несерьезных, например предложить модели денежное вознаграждение за хороший ответ, до довольно сложных, таких как chain of где модель просит порассуждать несколько шагов диалога перед тем, как дать финальный ответ, и это действительно дает буст в качестве и используется на проде. Другая проблема заключается в том, что модели ограничены теми знаниями, которые были у нее в обучающем корпусе, и эти знания всегда ограничены по времени. Ответы дала ЛМ о том, какой сейчас самый последний айфон будет одинаковым сейчас и даже через год. Как решить эту проблему? На помощь приходит раг, ретриевл, на помощь приходит раг, на помощь приходит раг, ретриевл, это техника заключается в том, что мы берем какую-то коллекцию документов, сжимаем их энкодером, например бертом и кладем в особую векторную базу данных, после этого запрос пользователя тоже векторизуется и к нему ищутся самые релевантные тексты, эти тексты подклеиваются к входным данным ЛЛМ, после чего она постарается дать, она не может стараться эта машина, нельзя душевлять инструментом, эти тексты потом подклеиваются к входному контексту ЛЛМ и она может использовать данные тексты, чтобы дать более правдивый и точный ответ, например популярное применение это общение с ПДФ, если вас завалило бюрократией или вам просто одиноко, можно пообщаться с коллекцией своих ПДФ документов, спросить, какой у вас баланс по счетам, что вам принадлежит и кому вы должны денег. Раг-система как выдаст и ответ с помощью ЛЛМ своими словами, так и сможет подсветить вам нужные параграфы текста, где содержалась релевантная информация. Другой способ обогатить модели знаниями или улучшить качество на своих задачах это файн тюнинг, модели можно и даже нужно дообучать, несмотря на их пугающие размеры. Можно обучать как всю модель, для этого нам скорее всего потребуется небольшой кластер от 8 до 10, а может быть даже и сотен видеокарт, или же можно дообучать легковестные добавки, адаптеры, например адаптеры это питюнинг, то есть блоки которые подаются на вход вместе с энкодингами токенов и по сути заменяют подобранную инструкцию или лора, это модификация линейных слоев, которая позволяет за счет небольших матриц изменить поведение модели. Существуют и продвинутые техники для обучения такие как преференс тюнинг или обучение на предпочтениях, для подобного обучения не нужны пары текстов, инструкции и ответ, а нужны пары ответов к одной инструкции, один из которых размечен как более предпочтительный чем другой. Подобные данные можно собирать на взаимодействии с пользователем, например давать ему два ответа и просить выбрать какой из них лучше. Таким образом мы можем дообучать модель в ходе ее жизненного цикла на взаимодействиях с юзерами. Все это время мы говорим про обучение модели, про их использование, но мы так и не подняли тему того как выбрать модель, как вообще понять насколько модель хороша и как сделать выбор осознанно. Все, я скоро вас отпущу, простите. На помощь приходят бенчмарки, они бывают разные, например сдача классификации, саморизации, по которым вполне себе понятны оффлайн метрики, это классические дискриминативные НЛП задачи. Но как оценить модель на задачи ассистента, как вообще можно оценить ответ на открытый вопрос. Можно попросить разметчика выдавать оценку от 1 до 10, однако это не очень эффективно, потому что человек плохо работает в абсолютных шкалах, гораздо лучше люди справляются с попарными сравнениями, можно давать два ответа на один и тот же вопрос, и тогда человек запросто выберет тот, который из них является предпочтительным или скажет что они одинаковые. Именно по такому принципу работают ЛЛМ арены, и выглядят они следующим образом, вы пишете какую-либо инструкцию или вопрос, после чего две анонимные модели выдают вам ответ, и вам нужно сказать лучше первая, вторая или ответы одинаковые. На таких попарных сравнениях строится система рейтинга, которая выливается в лидерборд моделей. Самая популярная арена на данный момент это арена LMSIS, и первые 20 строчек на ней заполнены закрытыми коммерческими решениями, после чего уже начинается опенсорс, который отстает где-то на 100 очков рейтинга. Выбрать модель для своей задачи можно исходя из разных критериев, можно смотреть на соотношение цены качества, можно смотреть на офлайн бенчмарки, если ваша задача похожа на нечто, на чем модель замерялась, например математика, программирование или классификация текстов, но рекомендуется попробовать модель на паре своих примеров и посмотреть на то, как она будет себя вести. Если ваша задача была в обучающей выборке, велик шанс получить хорошее качество из коробки без лишних усилий. Время ролика подходит к концу, а значит пора закругляться. Если вы хотите научиться работать с LLM, fine-tune их, строить рак системы, использовать агентов, а что дальше, господи, какая дырявая голова, не быть мне актером. Если вы хотите научиться работать с LLM, fine-tune их, строить рак системы, обучать агентов и ускорять инференс, при этом понимать, как все это работает под капотом, записывайтесь на наш курс, он так и называется Large Language Models, ссылка в описании. А на этом у меня все, и мы прощаемся, до новых встреч, нет, я забыл продаться за лайки, а на этом наше видео подходит к концу, задавайте вопросы в комментариях, если вам что-то было непонятно или вы хотите что-то узнать. Подписывайтесь на канал и ставьте лайк, если вам понравилось это видео, иначе не ставьте, хотя в целом. До новых встреч, все, стоп.\n"
        ]
    }
]
